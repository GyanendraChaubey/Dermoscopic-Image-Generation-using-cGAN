{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is intended to take the sketches and random labels in 1D and generate the image then generated image is feed to the \n",
    "Discriminator along with real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T21:29:07.826742Z",
     "iopub.status.busy": "2024-04-08T21:29:07.825972Z",
     "iopub.status.idle": "2024-04-08T21:29:08.495951Z",
     "shell.execute_reply": "2024-04-08T21:29:08.495008Z",
     "shell.execute_reply.started": "2024-04-08T21:29:07.826709Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T19:18:04.446936Z",
     "iopub.status.busy": "2024-04-08T19:18:04.446105Z",
     "iopub.status.idle": "2024-04-08T19:18:04.452689Z",
     "shell.execute_reply": "2024-04-08T19:18:04.451777Z",
     "shell.execute_reply.started": "2024-04-08T19:18:04.446903Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_label_generator():\n",
    "    random_vector = [0.] * 7\n",
    "    index = random.randint(0, 6)\n",
    "    random_vector[index] = 1.\n",
    "    return random_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T19:18:04.454111Z",
     "iopub.status.busy": "2024-04-08T19:18:04.453797Z",
     "iopub.status.idle": "2024-04-08T19:18:04.467576Z",
     "shell.execute_reply": "2024-04-08T19:18:04.466756Z",
     "shell.execute_reply.started": "2024-04-08T19:18:04.454080Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataLoader(Dataset):\n",
    "    def __init__(self, image_folder, label_csv, sketch_folder, transform=None):\n",
    "        self.transform = transform\n",
    "        self.labels_df = pd.read_csv(label_csv)\n",
    "        self.labels_df = self.labels_df.dropna()\n",
    "        existing_images = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "        filtered_labels = []\n",
    "        for idx, row in self.labels_df.iterrows():\n",
    "            image=row['image']+'.jpg'\n",
    "            if image in existing_images:\n",
    "                filtered_labels.append(row)\n",
    "        self.labels_df = pd.DataFrame(filtered_labels)\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.sketch_folder = sketch_folder\n",
    "        self.sketch_transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_name = self.labels_df.iloc[idx]['image'] + '.jpg'\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Load label\n",
    "        label = self.labels_df.iloc[idx, 1:].values.astype(float)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        # Randomly sample sketch\n",
    "        random_sketch_idx = random.randint(0, len(os.listdir(self.sketch_folder)) - 1)\n",
    "        sketch_name = os.listdir(self.sketch_folder)[random_sketch_idx]\n",
    "        sketch_path = os.path.join(self.sketch_folder, sketch_name)\n",
    "        sketch = Image.open(sketch_path).convert('L')\n",
    "        transformed_sketch = self.sketch_transform(sketch)\n",
    "        \n",
    "        # Generate random label image\n",
    "        randomlabels = random_label_generator()\n",
    "        random_label = torch.tensor(randomlabels, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label_tensor, transformed_sketch, random_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T19:18:04.470549Z",
     "iopub.status.busy": "2024-04-08T19:18:04.469949Z",
     "iopub.status.idle": "2024-04-08T19:18:04.483106Z",
     "shell.execute_reply": "2024-04-08T19:18:04.482180Z",
     "shell.execute_reply.started": "2024-04-08T19:18:04.470525Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/ConditionalGAN'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T19:18:04.493662Z",
     "iopub.status.busy": "2024-04-08T19:18:04.493328Z",
     "iopub.status.idle": "2024-04-08T19:18:31.902920Z",
     "shell.execute_reply": "2024-04-08T19:18:31.902129Z",
     "shell.execute_reply.started": "2024-04-08T19:18:04.493633Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "\n",
    "train_sketch_folder='/workspace/ConditionalGAN/Dataset/Train/Train_contours'\n",
    "train_image_folder='/workspace/ConditionalGAN/Dataset/Train/Train_data'\n",
    "train_label_csv='/workspace/ConditionalGAN/Dataset/Train/Train_labels.csv'\n",
    "\n",
    "test_sketch_folder='/workspace/ConditionalGAN/Dataset/Test/Test_contours'\n",
    "test_image_folder='/workspace/ConditionalGAN/Dataset/Test/Test_data'\n",
    "test_label_csv='/workspace/ConditionalGAN/Dataset/Test/Test_Labels.csv'\n",
    "\n",
    "\n",
    "train_dataset = CustomDataLoader(train_image_folder,train_label_csv,train_sketch_folder,transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataLoader(test_image_folder,test_label_csv,test_sketch_folder,transform=transform)\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T20:15:13.043892Z",
     "iopub.status.busy": "2024-04-08T20:15:13.043558Z",
     "iopub.status.idle": "2024-04-08T20:15:13.055761Z",
     "shell.execute_reply": "2024-04-08T20:15:13.054800Z",
     "shell.execute_reply.started": "2024-04-08T20:15:13.043869Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, label_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define the layers of the generator\n",
    "        self.fc = nn.Linear(791, 512)\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(hidden_dim, hidden_dim // 2, 4, 2, 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(hidden_dim // 2)\n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(hidden_dim // 2, hidden_dim // 4, 4, 2, 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(hidden_dim // 4)\n",
    "        self.conv_transpose3 = nn.ConvTranspose2d(hidden_dim // 4, 3, 4, 2, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise, label):\n",
    "        # Concatenate noise and label\n",
    "        #noise = noise.reshape(-1)\n",
    "        #print(noise.shape, label.shape)\n",
    "        size=noise.shape[0]\n",
    "        self.hidden_dim=size*2\n",
    "        #print(size)\n",
    "        noise = noise.view(size, -1)\n",
    "        #print(noise.shape)\n",
    "        x = torch.cat((noise, label), dim=1)\n",
    "        #print(x.shape)\n",
    "        #x = x.permute(1,0)\n",
    "        #print(x.shape)\n",
    "        # Pass the concatenated tensor through the fully connected layer\n",
    "        #print(\"Hi1\")\n",
    "        x = self.fc(x)\n",
    "        #print(x.shape)\n",
    "        #x = x.permute(0, 3, 1, 2)\n",
    "        #x = x.permute(0, 2, 3, 1)\n",
    "        #print(x.shape)\n",
    "        #print(\"Hi2\")\n",
    "        x = x.contiguous()\n",
    "        x = x.view(-1, self.hidden_dim, 16, 16)\n",
    "        #print(x.shape)\n",
    "        # Reshape the tensor for the convolutional layers\n",
    "        #x = x.view(-1, self.hidden_dim, 4, 4)\n",
    "        #x=x.reshape(512,self.hidden_dim, 128, 128)\n",
    "        #print(x.shape)\n",
    "        # Pass the tensor through the first transposed convolutional layer\n",
    "        x = self.batch_norm1(self.conv_transpose1(x))\n",
    "        x = self.tanh(x)\n",
    "        #print(x.shape)\n",
    "        # Pass the tensor through the second transposed convolutional layer\n",
    "        x = self.batch_norm2(self.conv_transpose2(x))\n",
    "        x = self.tanh(x)\n",
    "        #print(x.shape)\n",
    "        # Pass the tensor through the third transposed convolutional layer\n",
    "        x = self.conv_transpose3(x)\n",
    "        #print(x.shape)\n",
    "        # Apply the hyperbolic tangent function to the output\n",
    "        x = self.tanh(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T20:15:13.469971Z",
     "iopub.status.busy": "2024-04-08T20:15:13.469605Z",
     "iopub.status.idle": "2024-04-08T20:15:13.481934Z",
     "shell.execute_reply": "2024-04-08T20:15:13.480921Z",
     "shell.execute_reply.started": "2024-04-08T20:15:13.469944Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define the layers of the discriminator\n",
    "        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 4, 2, 1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 4, 2, 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(hidden_dim * 2)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 4, 2, 1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(hidden_dim * 4)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 4 * 4, 1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Concatenate image and label\n",
    "        #label = label.permute(0, 3, 1, 2)\n",
    "        #print(image.shape, label.shape)\n",
    "        #x = torch.cat((image, label), dim=1)\n",
    "        #x = torch.cat((image, random_label_image.unsqueeze(1).expand_as(image)), dim=1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x=image\n",
    "        \n",
    "        # Pass the concatenated tensor through the convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = x.view(-1, self.hidden_dim * 4 * 4)\n",
    "        #print(x.shape)\n",
    "        # Pass the flattened tensor through the fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x=F.sigmoid(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T20:15:14.186674Z",
     "iopub.status.busy": "2024-04-08T20:15:14.186265Z",
     "iopub.status.idle": "2024-04-08T20:15:14.206227Z",
     "shell.execute_reply": "2024-04-08T20:15:14.205077Z",
     "shell.execute_reply.started": "2024-04-08T20:15:14.186644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Generator and Discriminator\n",
    "G = Generator(input_dim=1, label_dim=7, hidden_dim=64).cuda()\n",
    "D = Discriminator(input_dim=3, hidden_dim=64).cuda()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer for the generator and discriminator\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T22:53:34.240788Z",
     "iopub.status.busy": "2024-04-08T22:53:34.240131Z",
     "iopub.status.idle": "2024-04-08T22:54:12.123746Z",
     "shell.execute_reply": "2024-04-08T22:54:12.122798Z",
     "shell.execute_reply.started": "2024-04-08T22:53:34.240754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:oeszwxqg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-deluge-4</strong> at: <a href='https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN/runs/oeszwxqg' target=\"_blank\">https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN/runs/oeszwxqg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240410_214137-oeszwxqg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:oeszwxqg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c0811e7541480dbec206bc86f19d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112915103634198, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/ConditionalGAN/wandb/run-20240410_234222-4akn6xtf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN/runs/4akn6xtf' target=\"_blank\">different-valley-5</a></strong> to <a href='https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN' target=\"_blank\">https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN/runs/4akn6xtf' target=\"_blank\">https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN/runs/4akn6xtf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gyanendrachaubeyproject/ConditionalGAN/runs/4akn6xtf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5a55c4c3d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"ConditionalGAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T21:36:10.443838Z",
     "iopub.status.busy": "2024-04-08T21:36:10.443404Z",
     "iopub.status.idle": "2024-04-08T21:36:10.448332Z",
     "shell.execute_reply": "2024-04-08T21:36:10.447352Z",
     "shell.execute_reply.started": "2024-04-08T21:36:10.443808Z"
    }
   },
   "outputs": [],
   "source": [
    "# wandb.config.update({\n",
    "#     \"input_dim\": input_dim,\n",
    "#     \"label_dim\": label_dim,\n",
    "#     \"hidden_dim\": hidden_dim,\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"num_epochs\": num_epochs,\n",
    "#     \"lr\": 0.0002,\n",
    "#     \"betas\": (0.5, 0.999)\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T21:28:55.073823Z",
     "iopub.status.busy": "2024-04-08T21:28:55.072972Z",
     "iopub.status.idle": "2024-04-08T21:28:55.081924Z",
     "shell.execute_reply": "2024-04-08T21:28:55.080760Z",
     "shell.execute_reply.started": "2024-04-08T21:28:55.073784Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train the cGAN\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i, (images, labels, sketches, random_labels) in enumerate(train_loader):\n",
    "#         #print(images.shape, labels.shape, sketches.shape, random_labels.shape)\n",
    "#         #print(\"Train Descriminator\")\n",
    "#         # Train the discriminator\n",
    "#         D.zero_grad()\n",
    "\n",
    "#         # Train on real images\n",
    "#         real_images = Variable(images.cuda())\n",
    "#         real_labels = Variable(labels.cuda())\n",
    "#         real_output = D(real_images, real_labels)\n",
    "#         #print(real_output)\n",
    "#         #real_loss = criterion(real_output, Variable(torch.ones(real_output.size(0)).cuda()))\n",
    "#         real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "#         real_loss.backward()\n",
    "\n",
    "#         # Train on fake images\n",
    "#         noise = Variable(sketches.cuda())\n",
    "#         fake_labels = Variable(random_labels.cuda())\n",
    "#         fake_images = G(noise, fake_labels)\n",
    "#         fake_output = D(fake_images.detach(), fake_labels)\n",
    "#         #fake_loss = criterion(fake_output, Variable(torch.zeros(fake_output.size(0)).cuda()))\n",
    "#         fake_loss = criterion(fake_output, torch.ones_like(fake_output))\n",
    "#         fake_loss.backward()\n",
    "\n",
    "#         # Update the discriminator weights\n",
    "#         D_loss = real_loss + fake_loss\n",
    "#         optimizer_D.step()\n",
    "        \n",
    "#         #print(\"Train Generator\")\n",
    "\n",
    "#         # Train the generator\n",
    "#         G.zero_grad()\n",
    "\n",
    "#         # Train on fake images\n",
    "#         noise = Variable(sketches.cuda())\n",
    "#         fake_labels = Variable(random_labels.cuda())\n",
    "#         fake_images = G(noise, fake_labels)\n",
    "#         fake_output = D(fake_images, fake_labels)\n",
    "#         #G_loss = criterion(fake_output, Variable(torch.ones(fake_output.size(0)).cuda()))\n",
    "#         G_loss = criterion(fake_output, torch.ones_like(fake_output))\n",
    "#         G_loss.backward()\n",
    "\n",
    "#         # Update the generator weights\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#     # Print the loss for each epoch\n",
    "#     if epoch % 100 == 0:\n",
    "#         print('Epoch [{}/{}], Step [{}/{}], D_loss: {:.4f}, G_loss: {:.4f}'\n",
    "#               .format(epoch+1, num_epochs, i+1, len(train_loader), D_loss.item(), G_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T22:54:22.564427Z",
     "iopub.status.busy": "2024-04-08T22:54:22.564063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n",
      "Train the generator\n",
      "Train the discriminator\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [64, 32, 4, 4], expected input[1, 62, 16, 16] to have 64 channels, but got 62 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1962/4123276438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msketches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfake_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1962/4238822267.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, noise, label)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Pass the tensor through the first transposed convolutional layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_transpose1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    956\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    957\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [64, 32, 4, 4], expected input[1, 62, 16, 16] to have 64 channels, but got 62 channels instead"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels, sketches, random_labels) in enumerate(train_loader):\n",
    "        # Train the discriminator\n",
    "        print(\"Train the discriminator\")\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Train on real images\n",
    "        real_images = Variable(images.cuda())\n",
    "        real_labels = Variable(labels.cuda())\n",
    "        real_output = D(real_images)\n",
    "        real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "        real_loss.backward()\n",
    "\n",
    "        # Train on fake images\n",
    "        noise = Variable(sketches.cuda())\n",
    "        fake_labels = Variable(random_labels.cuda())\n",
    "        fake_images = G(noise, fake_labels)\n",
    "        fake_output = D(fake_images.detach())\n",
    "        fake_loss = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "        fake_loss.backward()\n",
    "\n",
    "        # Update the discriminator weights\n",
    "        D_loss = real_loss + fake_loss\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator\n",
    "        print(\"Train the generator\")\n",
    "        G.zero_grad()\n",
    "\n",
    "        # Train on fake images\n",
    "        noise = Variable(sketches.cuda())\n",
    "        fake_labels = Variable(random_labels.cuda())\n",
    "        fake_images = G(noise, fake_labels)\n",
    "        fake_output = D(fake_images)\n",
    "        G_loss = criterion(fake_output, torch.ones_like(fake_output))\n",
    "        G_loss.backward()\n",
    "\n",
    "        # Update the generator weights\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # Log the training loss\n",
    "    wandb.log({\"epoch\": epoch+1, \"D_loss\": D_loss.item(), \"G_loss\": G_loss.item()})\n",
    "\n",
    "    # Perform validation/testing\n",
    "    test_loss_D = 0.0\n",
    "    test_loss_G = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for images, labels, sketches, random_labels in test_loader:\n",
    "        with torch.no_grad():\n",
    "            # Calculate discriminator loss on test set\n",
    "            print(\"calculate the descriminator\")\n",
    "            real_images = images.cuda()\n",
    "            real_labels = labels.cuda()\n",
    "            real_output = D(real_images)\n",
    "            real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "\n",
    "            noise = sketches.cuda()\n",
    "            fake_labels = random_labels.cuda()\n",
    "            fake_images = G(noise, fake_labels)\n",
    "            fake_output = D(fake_images)\n",
    "            fake_loss = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "\n",
    "            test_loss_D += real_loss.item() + fake_loss.item()\n",
    "\n",
    "            # Calculate generator loss on test set\n",
    "            print(\"calculate the generator\")\n",
    "            noise = sketches.cuda()\n",
    "            fake_labels = random_labels.cuda()\n",
    "            fake_images = G(noise, fake_labels)\n",
    "            fake_output = D(fake_images)\n",
    "            G_loss = criterion(fake_output, torch.ones_like(fake_output))\n",
    "            test_loss_G += G_loss.item()\n",
    "\n",
    "            total_batches += 1\n",
    "\n",
    "    # Average the test loss over all batches\n",
    "    avg_test_loss_D = test_loss_D / total_batches\n",
    "    avg_test_loss_G = test_loss_G / total_batches\n",
    "\n",
    "    # Log the test loss\n",
    "    wandb.log({\"epoch\": epoch+1, \"test_D_loss\": avg_test_loss_D, \"test_G_loss\": avg_test_loss_G})\n",
    "\n",
    "    # Print the loss for each epoch\n",
    "    print('Epoch [{}/{}], D_loss: {:.4f}, G_loss: {:.4f}, Test D_loss: {:.4f}, Test G_loss: {:.4f}'\n",
    "          .format(epoch+1, num_epochs, D_loss.item(), G_loss.item(), avg_test_loss_D, avg_test_loss_G))\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(G.state_dict(), 'generator.pth')\n",
    "torch.save(D.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "G_loaded = Generator(input_dim=1, label_dim=7, hidden_dim=64).cuda()\n",
    "D_loaded = Discriminator(input_dim=3, label_dim=7, hidden_dim=64).cuda()\n",
    "\n",
    "G_loaded.load_state_dict(torch.load('generator.pth'))\n",
    "D_loaded.load_state_dict(torch.load('discriminator.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize some sample images using test loader\n",
    "num_samples = 10\n",
    "test_iter = iter(test_loader)\n",
    "images, labels, sketches, random_labels = next(test_iter)\n",
    "\n",
    "# Move data to GPU if available\n",
    "images = images.cuda()\n",
    "random_labels = random_labels.cuda()\n",
    "\n",
    "# Generate images using the generator\n",
    "generated_images = G(images, random_labels)\n",
    "\n",
    "# Visualize the generated images\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(make_grid(generated_images.cpu().detach(), padding=2, normalize=True), (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4736065,
     "sourceId": 8034432,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
